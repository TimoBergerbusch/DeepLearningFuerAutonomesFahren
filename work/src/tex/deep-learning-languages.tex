\chapter{Deep Learning Languages}\label{chapter: DLL}

%TODO: elabortely <-/-> aufwendig, ausfÃ¼hrlich
Constructing a CNN from scratch in any typical language like Java, C++, or Python is very elaborately and has a high error potential. Even libraries in any such language often encounter the problem of over-complication due to their own style and syntactical and semantic architecture. Therefore there is a need of specialized languages.

The Deep Learning Languages (DLLs) are part of the Domain Specific Languages (DSL). Their main goal is to provide an easy to understand, as less verbose but as expressive as possible way of describing a CNN with its different layers and connections. Also one wants to have simple build presets

%SEE: maybe maintain the number
For that we consider three deep learning languages and analyse them on the previously mentioned properties.

\section{CNNArchLang}\label{sec: CNNArch}

(The whole description is based on\cite{CNNArch} and especially \cite{tim2018CNNArchLang})

One language for modeling CNNs is CNNArchLang. This language is developed at the Chair of Software Engineering, especially Thomas Michael Timmermanns,  at the RWTH Aachen University and part of the MontiCar language family. The main purpose of its creation is the necessity of special properties not given by other CNN-languages: \textit{C\&C integration} and \textit{type-safe component interface}. Its basic structure is very similar to python to improve understanding, based on familiarity with Python, and have an equal non-typed syntax. 

One very huge advantage of \cnnarch is that it's designed to be very simplistic and have less verbose %TODO: verbosity?
than most other languages to model CNNs. It does so, by moving from defining a CNN by every single neuron to the definition via layers only used a specialized notation, explained in the next paragraph. For that specific purpose many layers are already defined (c.f. \Cref{subsec: CNNArchLang - predefines layers}). 
New layers can be constructed by combining predefined layers.\\
This slightly reduces the expressiveness, since the possibility of performing computations on single tensors is lost. Such low-level operations are used extremely rarely and are not a drastic disadvantage.

In contrast to other languages for deep learning \cnnarch does not denote the connections of layers directly, but tries to model the data flow through the network. For that specific task it contains two main operators:
\begin{itemize}
	\item[-$>$:] Serial Connection:\label{item: sequential connection}\\
	This orders two elements sequentially. This means it denotes the first elements output as the second elements input. 
	\item[$|$:] Parallelization:\\
	This allows the split of the network into separate data streams, which can be computed in parallel.
\end{itemize}
Since serial connection has the higher precedence one has to use brackets. Also to merge the splitted streams, created by $|$, one can use the operators: \texttt{Concatenate()}, \texttt{Add()} and \texttt{Get(index)}.

%\subsection{General Definitions}\label{subsec: general definitions}
%The general definitions of a CNN, which are the input, i.e. an image, maybe additional data in a specific file type, i.e. sensor data, and the output dimension, denoting the predictions or in our example the actions the car should perform. Those are the only typed values within the CNN model.
%
%Such definitions can be modeled in \cnnarch as presented in \Cref{lst: general definitons CNNArchLang}.
%
%\begin{figure}[H]
%	\centering
%	\begin{lstlisting}[ basicstyle=\scriptsize]
%	def input Z(0:255)^{3, h, w} image[2]
%	def input Q(-oo:+oo)^{10} additionalData
%	def output Q(0:1)^{3} predictions
%	\end{lstlisting}
%	\caption{A general definition of a CNN using \cnnarch}
%	\label{lst: general definitons CNNArchLang}
%\end{figure}
%
%Further analyzed the definition can be broken down to the following components:
%\begin{itemize}
%	\item Keyword: \texttt{def}\\
%	Every input and output can be introduced using the keyword \texttt{def}
%	\item Direction: \texttt{input}/\texttt{output}\\
%	Every definition, being a part of the \ref{subsec: general definitions}, has to be defined to be either an \texttt{input} or an \texttt{output}
%	\item Range of numbers:\\
%	One can define the input to have special constraints. For example only integer values are denoted by a \texttt{Z} representing $\mathbb{Z}$. The same for \texttt{Q} and $\mathbb{Q}$. \\
%	Also the range has to be given via \texttt{(x:y)}, where \texttt{x} and \texttt{y} either are numbers or \texttt{-oo}( or \texttt{oo}) to denote $\infty$.
%	\item Size:\\
%	The size of for example the input or the number of classes is denoted by a matrix like notation using \texttt{\^{}\{size\}}  . For the input image (line 1) the size \texttt{\^{}\{3,h,w\}} determines the input image to have 3 channels with an image width of \texttt{w} and image height of \texttt{h}.
%	The others are just defined as 1$\times$10 or 1$\times$3 vectors/tensors.
%	\item Naming:\\
%	At the end of the line there has to be a name to identify the corresponding input/output.\\
%	Also through the \texttt{[2]} behind the name \texttt{image} one can define it to be a fixed length array of images.
%\end{itemize}

%\subsection{Array handling}
%An array, like in the example shown in \Cref{lst: general definitons CNNArchLang}, can be used either element wise:
%\begin{center}
%	\texttt{image[0]-$>$}
%\end{center}
%which would access the first image and create a sequential connection (c.f. \Cref{item: sequential connection}),
%whereas 
%\begin{center}
%	\texttt{image-$>$} $\equiv$ \texttt{(image[0] | image[1]) -$>$}
%\end{center}
%would result in two separate streams. If the receiving end of the sequential connection, for sake of example let it be called \texttt{out}, is also an array of size two one can also use
%\begin{center}
%	\texttt{image -$>$ out} $\equiv$  \texttt{([0]-$>$ out[0] | [1] -$>$ out[1])}
%\end{center}

\subsection{Predefined Layers and Functions} \label{subsec: CNNArchLang - predefines layers}
Different CNNs often use a similar basic set of layers, but arrange them differently. For that purpose there are some layers already defined by \cnnarch to simplify the usage.
There is for example the \texttt{FullyConnected}-layer with parameters for the number of units within and whether they should use a bias value (c.f. \Cref{sec: NN}), the \texttt{Convolutional}-2D-layer with parameters for the kernel (rectangle) size, number of filters, the stride defining the distance of two rectangles, padding and the usage of biases. Further information on any of these parameters or other predefined layers can be found in \cite{CNNArch}.

Also there are already defined functions like the \texttt{Sigmoid}, \texttt{Softmax}, \texttt{Tanh} or \texttt{ReLu}. 
One important aspect is that every argument has to be named. 

The distinction, that \cnnarch is not a framework itself, is very important. It is used to create the code to function in the \mxnet (see \Cref{sec: MxNet}).
%\todo{\begin{itemize}
%\item talk about types and checks (4.4)
%\item not all CNNs since no low-level operators
%\item as expressive as Caffe but < MxNet
%\item less verbose than Caffe example of page 50(57)
%\end{itemize}}

\section{Caffe} \label{sec: Caffe}

%information based on \cite{jia2014caffe}

\caffe is a full deep learning framework, created by Yangqing Jia during his PhD at UC Berkeley. It is a framework specially build to deal with multimedia input formats using state-of-the-art deep learning techniques. It comes as a BSD-license C++ library offering Python and MATLAB bindings. One of its reasons why it's so well known and frequently used is because of its design based on expressiveness, speed and modularity.

Using Nvidias Deep Neural Network library cuDNN as a wrapper of the CUDA functionality, \caffe can use the GPU in order to process even faster and learn in a rate of 40 million pictures per day. The possibility of using multiple GPUs in SLI is not stated and therefore not taken into account. \cite{wiki:Caffe}
However the possibility of using a cloud system is mentioned. But whether it is a simple decentralization or the possibility to train the network using the combined power of multiple computers to train is not mentioned. \cite{jia2014caffe}

\caffe tries to improve its readability by abstracting from the actual implementation using a graph-oriented way of network defining. For that \caffe uses two elements to represent the network:
\begin{itemize}
	\item \textbf{Blob}:\\
		A 4-dimensional array storing data, like images, parameters or parameter updates. These blob are the communication between layers.
	\item \textbf{Layers}:\\
		A Layer as described in \Cref{sec: NN} and \Cref{sec:CNN}.
\end{itemize}
The whole model gets saved as a Google Protocol Buffer, which is a language-neutral structurization, with important key features like size-minimal and efficient serialization and efficient interfaces for C++ and Python. \cite{jia2014caffe} \cite{varda2008protocol}
An example of a graph and its corresponding Protocol Buffer representation, written in prototxt, are given in \Cref{fig: Caffe Graph and Protobuf}.

\input{src/tex/caffe-graph-and-protobuf-example}

The biggest advantage of \caffe is the huge community providing a large set of presets, like layers or pre-trained nets and also a huge forum to ask other users about problems regarding ones project. This is such an advantage, because every module it is  guaranteed to pass a coverage test. \cite{jia2014caffe}

\caffe is written using as plain text named prototxt and therefore can be run via command line. One problem mentioned with that is that even though there are many nets already defined the creation of new nets often is highly verbose and repetitive. There are no shortenings. For example \cite{tim2018CNNArchLang} mentioned an example net written in \cnnarch with 36 lines and in \caffe with 6700 lines. This is also due to the fact that even if a layer can be constructed as a composition of existing layers one often has to define the forward-, backward and gradient-updates. %TODO ref

\section{Caffe2} \label{sec: Caffe2}
The framework \caffetwo is the successor of \caffe. \caffetwo is developed by Facebook and its current main usage is the phrase-wise translation in social networks. Keeping the modularity of \caffe in mind \caffetwo is also designed so that it can be up-scaled as well as mobile deployed. Also \caffetwo is designed in such a fashion that it can easily adapt to drastic changes like quantized computing. \cite{Caffe2Homepage}

Since the whole architecture is rewritten from scratch and regarding its now roughly one year of existence the library performs relatively well, but does not have the impact to outperform \caffe \cite{heise:Caffe2}.
One upside of the rewriting is that \caffetwo has a Python binding, like most other frameworks. One downside of the rewriting is that huge parts of the framework are not sufficiently documented. \caffetwo tries to improve but still has large holes within the documentation. \cite{tim2018CNNArchLang}

\caffetwo offers programs, which allow the user to convert \caffe and PyTorch models to \caffetwo. This makes the switching to \caffetwo much easier, since the users don't have to rewrite their models. \cite{Caffe2Homepage}

The main difference between \caffe and \caffetwo in terms of designing a \nn is that in \caffetwo the user uses \texttt{Operators} as the basic units instead of layers. Even though they are similar to the layers of \caffe, they are more flexible and adaptable. Partly based on the popularity of \caffe, \caffetwo also has a huge community and a large set of preset \texttt{Operators}, which can be used. \cite{Caffe2Homepage}

%\section{Tensorflow} \label{sec: Tensorflow}

\section{MxNet}\label{sec: MxNet}

Another framework often mentioned in research is the \mxnet. This deep learning framework is part of the Apache Software Foundation. Also it is said to be ``Amazons deep learning framework of choice'' \cite{infoworld} and featured to be a preset on the Amazon Web Services (AWS). \cite{chenmxnet}

The \mxnet tries to combine the advantages of imperative frameworks like numpy or MATLAB with the advantages of declarative frameworks like \caffe, \caffetwo or \tensorflow.

The advantages of imperative and declarative approaches can best be understood using an example.
Let the example be to compute: $a = b+c$
\begin{itemize}
	\item[] \textbf{imperative}:
		\begin{itemize}
			\setlength{\itemindent}{1.5 cm}
			\item[Procedure:] check the ability of $b$ and $c$ to be added. If so strictly compute the sum and declare $a$ as the same type as $b$ and $c$
			\item[Advantage:] very straightforward, works well with typical structures, debugger and third party libraries
			\item[Usefull for:] natural parameter updates and interactive debugging
		\end{itemize}
	\item[] \textbf{declarative}:
		\begin{itemize}
			\setlength{\itemindent}{1.5cm}
			\item[Procedure:] compute the computation graph and store the values of $b$ and $c$ as data bindings
			\item[Advantage:] perform computation as late as possible, leading to good optimization possibilities
			\item[Usefull for:] specifying computation structure, optimization
		\end{itemize}
\end{itemize}

By combining both approaches \mxnet is able to provide a superset programming interface compared to \caffe. \cite{chen2015mxnet}

Also \mxnet is able to reduce memory to a minimum by performing everything they can in-place and free or reuse as fast as possible. Thus the memory usage of \mxnet is outperforming \caffe. \cite{chen2015mxnet} % and Tensorflow
A new benchmark with \caffetwo has not been done yet.

An other very big upside of \mxnet is the possibility to use not only multiple GPUs in an SLI connection, but also multiple computers or even server to train a \nn simultaneously. This results in an outstanding scalability. \cite{chen2015mxnet}

Similar to \caffetwo, \mxnet also allows the deployment of trained models to low-end devices using Amalgamation (c.f. \cite{MxNetHomepage}) or the AWS.

Due to the fact that \mxnet has found its way into Apache Incubator and therefore it is an Open-Source project, the creation of additional functions and nets is quite simple and is not bound to a given preset. Thanks to the community also a variety of nets constructed and some already pre-trained are free to use. \cite{MxNetGithubModelGallery}
